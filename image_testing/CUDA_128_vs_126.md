Andrei — **CUDA 12.8 vs 12.6 is not “incompatible” in the abstract**, but there are two *very practical* compatibility axes that explain both the issues people hit and why PyTorch publishes **both** images.

## 1) The big compatibility lever is the **host NVIDIA driver**

In containers, the CUDA *runtime libraries* come from the image, but the **GPU driver always comes from the host**.

NVIDIA documents two relevant notions:

* **Toolkit “recommended/min” driver** per CUDA release (to get full support).
* **CUDA 12.x “minor version compatibility”**: CUDA apps built with a 12.x toolkit can often run on *older* 12.x-capable drivers (down to a 12.x baseline), **but with caveats**. ([NVIDIA Docs][1])

Concrete driver numbers (from CUDA release notes):

* CUDA **12.8 GA**: driver **>= 570.26 (Linux)** / **>= 570.65 (Windows)** ([NVIDIA Docs][1])
* CUDA **12.6 GA**: driver **>= 560.28.03 (Linux)** / **>= 560.76 (Windows)** ([NVIDIA Docs][1])

So if your fleet is pinned to **R560** drivers (common in enterprise/HPC), a **cu126** image is the “safe” match; cu128 may work via minor-version compatibility, but it’s not the least-surprising path.

### Minor-version compatibility caveats that matter in practice

NVIDIA explicitly calls out limitations like:

* **“Limited feature set”** when running with an older driver
* **PTX/JIT caveats** (PTX compiled by a newer toolkit may not work on older drivers)
* Potential errors like **“call requires newer driver”** if you hit a feature boundary ([NVIDIA Docs][2])

That’s a common root cause behind “works on one machine / fails on another” when mixing newer CUDA runtimes with older drivers.

## 2) The other huge lever: **which GPU architectures PyTorch includes in the binary**

PyTorch CUDA builds have to decide which **SMs** to ship kernels for. For newer CUDA lines (12.8/12.9+), PyTorch has *already* dropped some older architectures in those builds due to **binary size** and “moving forward” constraints.

Example from PyTorch release engineering discussion:

* A stated matrix (for a recent release line) shows **CUDA 12.6** builds including older SMs (e.g., **5.0/6.0/7.0**), while **CUDA 12.8** builds focus on newer SMs and include new ones (e.g., **10.0/12.0**) ([GitHub][3])
* PyTorch release notes also explicitly tell users: if you need **sm50–sm60**, use **CUDA 12.6** because CUDA 12.8/12.9 builds dropped them. ([GitHub][4])
* And PyTorch has announced **Volta will be removed from CUDA-12.8 builds** in an upcoming release because keeping Volta blocks cuDNN upgrades (cuDNN binaries dropping Volta). ([PyTorch Developer Mailing List][5])

So: **cu126 often exists to keep older GPUs working**, while **cu128 exists to enable newer GPUs and newer library stacks**.

## 3) Why PyTorch publishes both images (your two tags)

PyTorch is *actively* shipping binaries for multiple CUDA versions in the same release stream (including 12.6 and 12.8), so Docker images follow suit. ([PyTorch Developer Mailing List][6])

And Docker Hub shows the exact kind of parallel tags you listed for 2.10.0. ([Docker Hub][7])

Typical reasons:

1. **Driver reality / fleet constraints** (R560 vs R570+). ([NVIDIA Docs][1])
2. **GPU architecture coverage** (older GPUs often need cu126; newer GPUs may need cu128). ([GitHub][3])
3. **New GPU enablement**: PyTorch explicitly tied CUDA 12.8 wheels to Blackwell enablement. ([PyTorch][8])
4. **Ecosystem binary matching**: third-party CUDA extensions (flash-attn, xformers, etc.) usually publish wheels keyed to **cu126 vs cu128**, so PyTorch keeps both so you can stay consistent.

## Which one should you pick?

A practical decision rule:

* Choose **`…cuda12.8…`** if:

  * your host driver is **R570+**, and
  * your GPUs are **Turing/Ampere/Hopper/Blackwell+** (and you want the newer stack / newer GPU enablement). ([NVIDIA Docs][1])

* Choose **`…cuda12.6…`** if:

  * you’re pinned to **R560** drivers, and/or
  * you need older GPU support (historically: **Maxwell/Pascal/Volta** are the problem cases for newer CUDA builds). ([NVIDIA Docs][1])

### Quick self-check inside any container

Run:

```bash
nvidia-smi
python -c "import torch; print(torch.__version__); print('torch CUDA:', torch.version.cuda); print('arch list:', torch.cuda.get_arch_list())"
```

* `nvidia-smi` tells you the **host driver**.
* `torch.cuda.get_arch_list()` tells you which SMs the **PyTorch binary actually contains** (this is the fastest way to confirm “will my GPU work?”).

**Confidence: 90% — because** NVIDIA documents the driver/compat rules for CUDA 12.x, and PyTorch’s own release engineering notes/issues explicitly justify keeping cu126 alongside cu128.

[1]: https://docs.nvidia.com/cuda/archive/12.8.0/cuda-toolkit-release-notes/ "1. CUDA 12.8 Release Notes — Release Notes 12.8 documentation"
[2]: https://docs.nvidia.com/deploy/cuda-compatibility/minor-version-compatibility.html "Minor Version Compatibility — CUDA Compatibility"
[3]: https://github.com/pytorch/pytorch/issues/157517 "[release 2.8-2.9] Delete support for Maxwell, Pascal, and Volta architectures for CUDA 12.8 and 12.9 builds · Issue #157517 · pytorch/pytorch · GitHub"
[4]: https://github.com/pytorch/pytorch/releases "Releases · pytorch/pytorch · GitHub"
[5]: https://dev-discuss.pytorch.org/t/dropping-volta-support-from-cuda-12-8-binaries-for-release-2-11/3290?utm_source=chatgpt.com "Dropping Volta support from CUDA-12.8 binaries for ..."
[6]: https://dev-discuss.pytorch.org/t/pytorch-2-10-rc1-produced-for-pytorch-torchvision-torchaudio/3281?utm_source=chatgpt.com "PyTorch 2.10 RC1 produced for pytorch, torchvision & ..."
[7]: https://hub.docker.com/r/pytorch/pytorch/tags?utm_source=chatgpt.com "pytorch/pytorch - Docker Image"
[8]: https://pytorch.org/blog/pytorch-2-7/?utm_source=chatgpt.com "PyTorch 2.7 Release"


=========================



“**SM 12**” in NVIDIA terms normally means **compute capability 12.0**, i.e. **`sm_120`** — used by **GeForce RTX 5090/5080/5070 Ti/...** and RTX PRO Blackwell variants. ([NVIDIA Developer][1])

### If you mean “can I *compile* for SM 12.0 with CUDA 12.6?”

**No.** NVCC/toolchain support for **`sm_120`** was added with **CUDA 12.8** (“adds compiler support for … SM_120”). ([NVIDIA Docs][2])
So **CUDA 12.6 cannot generate native `sm_120` cubins** (and most CUDA extensions you build with 12.6 won’t be able to target `sm_120` explicitly).

### If you mean “can a CUDA 12.6-built app *run* on SM 12.0?”

**Sometimes, but only under specific conditions**:

* **PTX is forward-compatible**: PTX for an older compute target can be JIT-compiled by the **driver** for newer GPUs (NVIDIA explicitly uses **`sm_120`** as an example of a later target). ([NVIDIA Docs][3])
* **If the binary has no compatible cubin and no PTX**, kernel launch fails (“no kernel image…”). ([NVIDIA Docs][3])

So in principle, a CUDA 12.6-era binary **can** run on `sm_120` *if it ships PTX* — but that’s an application packaging choice, not guaranteed.

### For PyTorch specifically (your context: `+cu126` vs `+cu128`)

In practice, **don’t expect `+cu126` PyTorch images/wheels to work on `sm_120`**. The PyTorch maintainers have repeatedly pointed Blackwell users to **CUDA 12.8+ builds** for support. ([PyTorch Forums][4])

So: **Blackwell (sm_120) → pick the `cuda12.8` (or newer) PyTorch image**.

### Quick sanity check

Inside the container:

```bash
python -c "import torch; print(torch.version.cuda); print(torch.cuda.get_arch_list())"
```

* If `sm_120` is not in the arch list, you’re very likely to hit the incompatibility / “no kernel image” behavior on RTX 50-series.

**Confidence: 92% — because** NVIDIA’s CUDA release docs explicitly say `sm_120` compiler support appears in CUDA 12.8, and PyTorch maintainers directly state Blackwell support comes via CUDA 12.8+ builds. ([NVIDIA Docs][2])

[1]: https://developer.nvidia.com/cuda/gpus "CUDA GPU Compute Capability | NVIDIA Developer"
[2]: https://docs.nvidia.com/cuda/archive/12.9.1/cuda-features-archive/index.html "CUDA Features Archive — CUDA Features Archive 12.9 documentation"
[3]: https://docs.nvidia.com/cuda/cuda-programming-guide/01-introduction/cuda-platform.html "1.3. The CUDA platform — CUDA Programming Guide"
[4]: https://discuss.pytorch.org/t/when-will-sm120-support-be-available/223621 "When will sm120 support be available? - PyTorch Forums"
